{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa Training for Multilingual Sentiment Analysis\n",
    "\n",
    "This notebook trains XLM-RoBERTa on the English Financial PhraseBank dataset\n",
    "and tests cross-lingual transfer to Spanish.\n",
    "\n",
    "**Model:** `xlm-roberta-base` (270M parameters, 100+ languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from config import XLM_ROBERTA_CONFIG, print_config, MODELS_DIR\n",
    "from config.params import ID2LABEL\n",
    "from src.data import load_financial_phrasebank, create_data_splits, create_dataloaders\n",
    "from src.models import create_model, Trainer, ModelEvaluator, SentimentPredictor\n",
    "from src.models.classifier import print_model_info\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show configuration\n",
    "print_config(XLM_ROBERTA_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading Financial PhraseBank...\")\n",
    "df = load_financial_phrasebank(agreement_level=\"sentences_75agree\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "print(\"Creating train/val/test splits...\")\n",
    "train_df, val_df, test_df = create_data_splits(df, seed=42)\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders with XLM-RoBERTa tokenizer\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_df, val_df, test_df,\n",
    "    tokenizer_name=XLM_ROBERTA_CONFIG.model_checkpoint,\n",
    "    batch_size=XLM_ROBERTA_CONFIG.batch_size,\n",
    "    max_length=XLM_ROBERTA_CONFIG.max_seq_length,\n",
    "    num_workers=0  # Windows compatibility\n",
    ")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"Creating XLM-RoBERTa model...\")\n",
    "model = create_model(\n",
    "    model_checkpoint=XLM_ROBERTA_CONFIG.model_checkpoint,\n",
    "    num_labels=3,\n",
    "    device=device\n",
    ")\n",
    "print_model_info(model, \"XLM-RoBERTa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer.from_config(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=XLM_ROBERTA_CONFIG,\n",
    "    save_dir=str(MODELS_DIR / \"xlm-roberta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['val_accuracy'], label='Validation', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation on English Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "from src.models import load_model\n",
    "\n",
    "best_model_path = MODELS_DIR / \"xlm-roberta\" / \"best_model.pt\"\n",
    "model = load_model(\n",
    "    checkpoint_path=str(best_model_path),\n",
    "    model_checkpoint=XLM_ROBERTA_CONFIG.model_checkpoint,\n",
    "    num_labels=3,\n",
    "    device=device\n",
    ")\n",
    "print(f\"Loaded best model from {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "evaluator = ModelEvaluator(model, device=device)\n",
    "metrics, predictions, labels = evaluator.evaluate(test_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS (English)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Macro:  {metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1 Weighted: {metrics['f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('XLM-RoBERTa - Confusion Matrix (English Test Set)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, predictions, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-lingual Testing (Spanish)\n",
    "\n",
    "Test zero-shot transfer: model trained on English, tested on Spanish financial texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "predictor = SentimentPredictor(\n",
    "    model_path=str(best_model_path),\n",
    "    tokenizer_name=XLM_ROBERTA_CONFIG.model_checkpoint,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish financial test sentences\n",
    "spanish_texts = [\n",
    "    # Positive\n",
    "    (\"Los ingresos de la empresa aumentaron un 25% en el tercer trimestre.\", \"positive\"),\n",
    "    (\"La compañía reportó ganancias récord este año.\", \"positive\"),\n",
    "    (\"Las acciones subieron tras el anuncio de dividendos.\", \"positive\"),\n",
    "    (\"El nuevo producto superó las expectativas del mercado.\", \"positive\"),\n",
    "    (\"La fusión generará sinergias significativas.\", \"positive\"),\n",
    "    \n",
    "    # Negative\n",
    "    (\"La empresa anunció pérdidas significativas en el último trimestre.\", \"negative\"),\n",
    "    (\"Las ventas cayeron un 15% debido a la competencia.\", \"negative\"),\n",
    "    (\"El CEO renunció tras el escándalo financiero.\", \"negative\"),\n",
    "    (\"La compañía recortará 500 empleos para reducir costos.\", \"negative\"),\n",
    "    (\"Los inversores perdieron confianza en la gestión.\", \"negative\"),\n",
    "    \n",
    "    # Neutral\n",
    "    (\"La empresa publicará sus resultados el próximo lunes.\", \"neutral\"),\n",
    "    (\"El consejo de administración se reunirá mañana.\", \"neutral\"),\n",
    "    (\"La compañía tiene sede en Madrid.\", \"neutral\"),\n",
    "    (\"El informe anual estará disponible en línea.\", \"neutral\"),\n",
    "    (\"La empresa opera en el sector tecnológico.\", \"neutral\"),\n",
    "]\n",
    "\n",
    "print(f\"Testing on {len(spanish_texts)} Spanish sentences...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "results = []\n",
    "for text, true_label in spanish_texts:\n",
    "    pred = predictor.predict(text)\n",
    "    results.append({\n",
    "        'text': text[:60] + '...' if len(text) > 60 else text,\n",
    "        'true': true_label,\n",
    "        'predicted': pred['label'],\n",
    "        'confidence': pred['confidence'],\n",
    "        'correct': pred['label'] == true_label\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spanish accuracy\n",
    "spanish_accuracy = results_df['correct'].mean()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SPANISH ZERO-SHOT RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {spanish_accuracy:.1%} ({results_df['correct'].sum()}/{len(results_df)})\")\n",
    "print(f\"\\nPer-class accuracy:\")\n",
    "for label in ['positive', 'neutral', 'negative']:\n",
    "    subset = results_df[results_df['true'] == label]\n",
    "    acc = subset['correct'].mean()\n",
    "    print(f\"  {label}: {acc:.1%} ({subset['correct'].sum()}/{len(subset)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare English vs Spanish\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON: English vs Spanish\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"English test accuracy: {metrics['accuracy']:.1%}\")\n",
    "print(f\"Spanish zero-shot:     {spanish_accuracy:.1%}\")\n",
    "print(f\"Transfer efficiency:   {spanish_accuracy/metrics['accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text: str):\n",
    "    \"\"\"Analyze sentiment of a text in any language.\"\"\"\n",
    "    result = predictor.predict(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label'].upper()}\")\n",
    "    print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "    print(f\"Probabilities: neg={result['probabilities']['negative']:.2f}, \"\n",
    "          f\"neu={result['probabilities']['neutral']:.2f}, \"\n",
    "          f\"pos={result['probabilities']['positive']:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Test with different languages\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTILINGUAL DEMO\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# English\n",
    "analyze_sentiment(\"Revenue increased by 25% this quarter.\")\n",
    "\n",
    "# Spanish\n",
    "analyze_sentiment(\"Los beneficios cayeron un 10% este año.\")\n",
    "\n",
    "# Russian (bonus test)\n",
    "analyze_sentiment(\"Компания объявила о рекордной прибыли.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own text!\n",
    "# analyze_sentiment(\"Your text here in any language\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
